# üéß F.R.I.D.A.Y. - Support Client

> **Female Replacement Intelligent Digital Assistant Youth**  
> Niveau : 1 | Status : ONLINE | Priorit√© : HAUTE

## üìã Mission

FRIDAY est le **premier contact client**. Elle g√®re les demandes entrantes, r√©pond aux questions fr√©quentes gr√¢ce au RAG, et escalade vers les humains quand n√©cessaire.

### Responsabilit√©s

- üí¨ **R√©ponses automatis√©es** : FAQ, infos de base, tarifs
- üé´ **Triage des tickets** : Classification et priorisation
- üìö **Base de connaissances** : RAG sur la documentation GL Digital Lab
- üîÑ **Escalade intelligente** : D√©tection des cas complexes
- üìà **Suivi satisfaction** : Collecte des feedbacks

---

## üõ†Ô∏è Stack Technique

| Composant | Technologie | R√¥le |
|-----------|-------------|------|
| Interface | **OpenWebUI** | Chat client-facing |
| LLM | **Ollama + Mistral** | G√©n√©ration de r√©ponses |
| RAG | **ChromaDB** | Base de connaissances vectorielle |
| Embeddings | **nomic-embed-text** | Vectorisation des documents |
| Orchestration | **n8n** | Workflows et int√©grations |

---

## üì¶ Installation

### Pr√©requis

```bash
# Ollama avec les mod√®les n√©cessaires
ollama pull mistral
ollama pull nomic-embed-text

# Python pour les scripts d'indexation
python3 --version  # >= 3.10
pip install chromadb langchain-community
```

### 1. Structure des dossiers

```bash
mkdir -p ~/gl-tower/friday/{config,data,knowledge}
cd ~/gl-tower/friday
```

### 2. Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  # OpenWebUI - Interface Chat
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: friday-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_AUTH=true
      - WEBUI_NAME=FRIDAY Support
      - DEFAULT_MODELS=mistral
      - RAG_EMBEDDING_MODEL=nomic-embed-text
      - CHROMA_HTTP_HOST=friday-chroma
      - CHROMA_HTTP_PORT=8000
    volumes:
      - ./data/openwebui:/app/backend/data
    networks:
      - gl-tower
    depends_on:
      - chromadb

  # ChromaDB - Vector Store
  chromadb:
    image: chromadb/chroma:latest
    container_name: friday-chroma
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_AUTH_TOKEN}
      - CHROMA_SERVER_AUTH_PROVIDER=chromadb.auth.token.TokenAuthServerProvider
    volumes:
      - ./data/chroma:/chroma/chroma
    networks:
      - gl-tower

networks:
  gl-tower:
    external: true
```

### 3. Variables d'environnement

```bash
# .env
CHROMA_AUTH_TOKEN=your-chroma-auth-token
FRIDAY_WEBHOOK=https://discord.com/api/webhooks/xxx
OLLAMA_HOST=http://host.docker.internal:11434
```

### 4. Lancement

```bash
docker compose up -d
```

---

## üìö Configuration du RAG

### 1. Pr√©parer les documents

Structure de la base de connaissances :

```
knowledge/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ performance.md
‚îÇ   ‚îú‚îÄ‚îÄ digital-factory.md
‚îÇ   ‚îî‚îÄ‚îÄ neural-ops.md
‚îú‚îÄ‚îÄ faq/
‚îÇ   ‚îú‚îÄ‚îÄ general.md
‚îÇ   ‚îú‚îÄ‚îÄ pricing.md
‚îÇ   ‚îî‚îÄ‚îÄ process.md
‚îú‚îÄ‚îÄ legal/
‚îÇ   ‚îú‚îÄ‚îÄ cgv.md
‚îÇ   ‚îî‚îÄ‚îÄ confidentialite.md
‚îî‚îÄ‚îÄ technical/
    ‚îú‚îÄ‚îÄ stack.md
    ‚îî‚îÄ‚îÄ integrations.md
```

### 2. Script d'indexation

```python
# index_knowledge.py
import os
import chromadb
from chromadb.config import Settings
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import OllamaEmbeddings

# Configuration
KNOWLEDGE_DIR = "./knowledge"
CHROMA_HOST = "localhost"
CHROMA_PORT = 8000
COLLECTION_NAME = "friday_knowledge"

def index_documents():
    # Connexion √† ChromaDB
    client = chromadb.HttpClient(
        host=CHROMA_HOST,
        port=CHROMA_PORT,
        settings=Settings(anonymized_telemetry=False)
    )
    
    # Cr√©er ou r√©cup√©rer la collection
    collection = client.get_or_create_collection(
        name=COLLECTION_NAME,
        metadata={"description": "FRIDAY Knowledge Base"}
    )
    
    # Charger les documents
    loader = DirectoryLoader(
        KNOWLEDGE_DIR,
        glob="**/*.md",
        loader_cls=TextLoader
    )
    documents = loader.load()
    
    # D√©couper en chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = splitter.split_documents(documents)
    
    # Embeddings avec Ollama
    embeddings = OllamaEmbeddings(
        model="nomic-embed-text",
        base_url="http://localhost:11434"
    )
    
    # Indexer les chunks
    for i, chunk in enumerate(chunks):
        embedding = embeddings.embed_query(chunk.page_content)
        collection.add(
            ids=[f"doc_{i}"],
            embeddings=[embedding],
            documents=[chunk.page_content],
            metadatas=[{
                "source": chunk.metadata.get("source", "unknown"),
                "chunk_index": i
            }]
        )
    
    print(f"Indexed {len(chunks)} chunks from {len(documents)} documents")

if __name__ == "__main__":
    index_documents()
```

### 3. Ex√©cuter l'indexation

```bash
pip install chromadb langchain-community
python index_knowledge.py
```

---

## üîÑ Workflows n8n

### Workflow 1 : Support Ticket Handler

```json
{
  "name": "FRIDAY - Support Ticket",
  "nodes": [
    {
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "path": "friday/ticket",
        "httpMethod": "POST"
      }
    },
    {
      "name": "Query RAG",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://friday-chroma:8000/api/v1/collections/friday_knowledge/query",
        "method": "POST",
        "body": {
          "query_texts": ["{{ $json.question }}"],
          "n_results": 3
        }
      }
    },
    {
      "name": "Generate Response",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "={{ $env.OLLAMA_HOST }}/api/generate",
        "method": "POST",
        "body": {
          "model": "mistral",
          "prompt": "Tu es FRIDAY, l'assistante support de GL Digital Lab. R√©ponds √† cette question en utilisant le contexte fourni.\n\nContexte:\n{{ $json.documents }}\n\nQuestion: {{ $node['Webhook Trigger'].json.question }}\n\nR√©ponds de mani√®re professionnelle et concise.",
          "stream": false
        }
      }
    },
    {
      "name": "Check Confidence",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "const response = $input.first().json.response;\nconst distances = $node['Query RAG'].json.distances[0];\n\n// Si la distance moyenne est trop grande, escalader\nconst avgDistance = distances.reduce((a,b) => a+b, 0) / distances.length;\nconst confident = avgDistance < 0.5;\n\nreturn [{ json: { response, confident, avgDistance } }];"
      }
    },
    {
      "name": "Route Response",
      "type": "n8n-nodes-base.if",
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.confident }}",
              "value2": true
            }
          ]
        }
      }
    }
  ]
}
```

### Workflow 2 : FAQ Auto-Reply Discord

```json
{
  "name": "FRIDAY - Discord FAQ",
  "nodes": [
    {
      "name": "Discord Trigger",
      "type": "n8n-nodes-base.discord",
      "parameters": {
        "event": "messageCreate",
        "channelId": "{{ $env.SUPPORT_CHANNEL_ID }}"
      }
    },
    {
      "name": "Filter Questions",
      "type": "n8n-nodes-base.if",
      "parameters": {
        "conditions": {
          "string": [
            {
              "value1": "={{ $json.content }}",
              "operation": "contains",
              "value2": "?"
            }
          ]
        }
      }
    },
    {
      "name": "Query Knowledge",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://friday-chroma:8000/api/v1/collections/friday_knowledge/query",
        "method": "POST",
        "body": {
          "query_texts": ["{{ $json.content }}"],
          "n_results": 2
        }
      }
    },
    {
      "name": "Generate FAQ Response",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "={{ $env.OLLAMA_HOST }}/api/generate",
        "method": "POST",
        "body": {
          "model": "mistral",
          "prompt": "Tu es FRIDAY. R√©ponds bri√®vement √† cette question Discord.\n\nContexte: {{ $json.documents }}\nQuestion: {{ $node['Discord Trigger'].json.content }}\n\nR√©ponds en 2-3 phrases max. Si tu n'es pas s√ªre, dis 'Je vais transmettre ta question √† l'√©quipe.'",
          "stream": false
        }
      }
    },
    {
      "name": "Reply Discord",
      "type": "n8n-nodes-base.discord",
      "parameters": {
        "webhookUri": "={{ $env.FRIDAY_WEBHOOK }}",
        "content": "üéß **FRIDAY** : {{ $json.response }}"
      }
    }
  ]
}
```

### Workflow 3 : Satisfaction Survey

```json
{
  "name": "FRIDAY - Satisfaction Survey",
  "nodes": [
    {
      "name": "Cron - Daily 18h",
      "type": "n8n-nodes-base.cron",
      "parameters": {
        "cronExpression": "0 18 * * *"
      }
    },
    {
      "name": "Get Today's Tickets",
      "type": "n8n-nodes-base.postgres",
      "parameters": {
        "query": "SELECT * FROM tickets WHERE status = 'resolved' AND resolved_at > NOW() - INTERVAL '24 hours'"
      }
    },
    {
      "name": "Send Survey",
      "type": "n8n-nodes-base.discord",
      "parameters": {
        "webhookUri": "={{ $env.FRIDAY_WEBHOOK }}",
        "content": "üéß **FRIDAY Survey**\n\nHey {{ $json.user_name }} ! Ta demande \"{{ $json.title }}\" a √©t√© r√©solue.\n\nComment √©values-tu notre support ?\nüëç Satisfait | üëé Pas satisfait | üí¨ Commentaire"
      }
    }
  ]
}
```

---

## üìö Contenu de la Knowledge Base

### Exemple : services/performance.md

```markdown
# Service Performance

## Description
Le service Performance de GL Digital Lab est un audit complet de votre infrastructure web, suivi d'optimisations cibl√©es.

## Inclus
- Audit Lighthouse complet (Performance, SEO, A11Y, Best Practices)
- Analyse Core Web Vitals
- Refonte Vue 3 / React si n√©cessaire
- Optimisation SSR et mise en cache
- Rapport PDF d√©taill√©

## Tarifs
- Audit seul : 2 000‚Ç¨ HT
- Audit + Optimisations : 8 000‚Ç¨ - 15 000‚Ç¨ HT selon complexit√©

## D√©lais
- Audit : 1 semaine
- Optimisations : 2-4 semaines

## Contact
R√©servez un audit gratuit de 30 minutes : contact@gldigitallab.fr
```

### Exemple : faq/general.md

```markdown
# FAQ G√©n√©rale

## O√π √™tes-vous bas√©s ?
GL Digital Lab est bas√© √† Limoges, en Nouvelle-Aquitaine (87). Nous travaillons avec des clients dans toute la France, principalement en remote.

## Quels sont vos horaires ?
Du lundi au vendredi, 9h-18h. R√©ponse garantie sous 24h ouvr√©es.

## Travaillez-vous avec des PME ?
Oui ! Notre cible principale est les PME de 10-100 salari√©s qui veulent reprendre le contr√¥le de leur infrastructure num√©rique.

## Proposez-vous du support apr√®s livraison ?
Oui, tous nos projets incluent 3 mois de support. Des contrats de maintenance sont disponibles ensuite.

## Quelles technologies utilisez-vous ?
- Backend : Symfony 8, PHP 8.3+
- Frontend : Vue 3, Three.js, Vite
- IA : Ollama (local), n8n, ChromaDB
- DevOps : Docker, GitHub Actions
```

---

## üìä M√©triques

FRIDAY expose ses m√©triques :

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'friday'
    static_configs:
      - targets: ['localhost:3000']
```

### KPIs suivis

| M√©trique | Description | Objectif |
|----------|-------------|----------|
| `friday_tickets_total` | Nombre de tickets trait√©s | +10%/mois |
| `friday_response_time_avg` | Temps de r√©ponse moyen | < 30s |
| `friday_satisfaction_rate` | Taux de satisfaction | > 90% |
| `friday_escalation_rate` | Taux d'escalade | < 20% |
| `friday_rag_confidence_avg` | Confiance RAG moyenne | > 0.7 |

---

## üîß Maintenance

### Mise √† jour de la Knowledge Base

```bash
# Ajouter de nouveaux documents
cp nouveau-doc.md ~/gl-tower/friday/knowledge/

# R√©-indexer
python index_knowledge.py

# V√©rifier l'indexation
curl http://localhost:8000/api/v1/collections/friday_knowledge | jq
```

### Backup ChromaDB

```bash
#!/bin/bash
BACKUP_DIR=~/backups/friday/$(date +%Y%m%d)
mkdir -p $BACKUP_DIR

docker exec friday-chroma tar czf /tmp/chroma-backup.tar.gz /chroma/chroma
docker cp friday-chroma:/tmp/chroma-backup.tar.gz $BACKUP_DIR/

echo "Backup complete: $BACKUP_DIR"
```

---

## üö® Troubleshooting

### RAG ne trouve pas de r√©sultats

```bash
# V√©rifier que la collection existe
curl http://localhost:8000/api/v1/collections | jq

# V√©rifier le nombre de documents
curl http://localhost:8000/api/v1/collections/friday_knowledge | jq '.count'

# R√©-indexer si n√©cessaire
python index_knowledge.py
```

### OpenWebUI ne d√©marre pas

```bash
# V√©rifier les logs
docker logs friday-webui --tail 50

# V√©rifier la connexion Ollama
curl http://localhost:11434/api/tags
```

---

## üìö Ressources

- [OpenWebUI Documentation](https://docs.openwebui.com/)
- [ChromaDB Documentation](https://docs.trychroma.com/)
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)

---

*Derni√®re mise √† jour : Janvier 2026*  
*Agent : FRIDAY v1.0 | GL Tower - Niveau 1*
